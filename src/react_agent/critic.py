"""Define a Critic Agent for evaluating Odoo 18 generated code.

This module implements a code evaluation agent using OpenEvals to assess
code quality, correctness, and adherence to Odoo standards.
"""

import asyncio
import time
import re
import json
from typing import Dict, List, Optional, Any, Callable, Awaitable, Union, Tuple

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.language_models import BaseChatModel
from openevals.llm import create_llm_as_judge

from react_agent.configuration import Configuration
from react_agent.state import State
from react_agent.utils import load_chat_model
# Import prompts from prompts.py
from react_agent.prompts import ODOO_CODE_QUALITY_PROMPT, ODOO_CORRECTNESS_PROMPT

# Import tools for critic access
from react_agent.tools import TOOLS, search
from react_agent.odoo_code_utils import (
    search_odoo_code,
    load_odoo_code,
    validate_odoo_code,
    generate_odoo_snippet,
    list_odoo_modules
)

# Get configuration
config = Configuration.from_context()

async def evaluate_code(state: State) -> Dict[str, List[AIMessage]]:
    """Evaluate code generated by the agent using OpenEvals and specialized Odoo evaluation.
    
    This function extracts code from the agent's message, evaluates it for quality and correctness,
    and provides detailed feedback with Odoo-specific recommendations.
    
    Args:
        state (State): The current state containing the agent's messages and code.
        
    Returns:
        Dict[str, List[AIMessage]]: Evaluation results and feedback messages.
    """
    # Track that we're in the critic node
    state.last_node = 'critic'
    if hasattr(state, 'node_history'):
        state.node_history.append('critic')
    else:
        state.node_history = ['critic']
        
    # Update model status to show we're starting evaluation
    from react_agent.utils import update_model_status
    update_model_status("critic", "system", "evaluator", "initializing")
    
    # Extract code to evaluate from the last message
    messages = state.messages if hasattr(state, 'messages') else []
    
    # Check if we have any messages to evaluate
    if not messages:
        # Create a default error message and return early
        error_message = "No messages found to evaluate. Please provide code to review."
        print(f"Critic Error: {error_message}")
        return {
            "messages": [AIMessage(content=error_message)],
            "is_last_step": True
        }
    
    # Safely get the last message
    last_message = messages[-1]
    code_to_evaluate = last_message.content

    # Extract code blocks from the message content
    code_blocks = extract_code_from_messages([last_message])
    
    # Check if any code blocks were found
    if not code_blocks:
        # Create a message indicating no code was found to evaluate
        no_code_message = "No code blocks found to evaluate. Please provide Odoo code in a code block format for review."
        print(f"Critic Warning: {no_code_message}")
        return {
            "messages": [AIMessage(content=no_code_message)],
            "is_last_step": False  # Allow the conversation to continue
        }
        
    # Check if the code looks like Odoo code (very basic validation)
    combined_code = "\n".join(code_blocks)
    if len(combined_code) < 10:  # Arbitrary minimum length for valid code
        # Code is too short to be meaningful
        invalid_code_message = "The provided code is too short to be meaningful. Please provide complete Odoo code for evaluation."
        print(f"Critic Warning: {invalid_code_message}")
        return {
            "messages": [AIMessage(content=invalid_code_message)],
            "is_last_step": False
        }
        
    # Check for some basic Odoo indicators in the code
    odoo_indicators = ['odoo', 'models.Model', 'fields.', '_inherit', '_name', 'self.env']
    if not any(indicator in combined_code.lower() for indicator in odoo_indicators):
        # Code may not be Odoo-related, but we'll still evaluate it with a warning
        print("Critic Warning: Code may not be Odoo-related, but proceeding with evaluation anyway.")


    # Initialize evaluators with configured model and fallback
    # Define a wrapper function to create evaluators with better error handling
    async def custom_llm_as_judge(prompt: str, model_name: str) -> Callable[[str], Awaitable[Dict[str, Union[float, str]]]]:
        """
        Custom implementation of llm_as_judge that handles models that don't support bind_tools().
        This version also gives the critic access to tools for Odoo codebase search and online search.
        
        Args:
            prompt: The evaluation prompt template
            model_name: The name of the model to use
            
        Returns:
            An async function that takes code as input and returns evaluation results
        """
        # Load the chat model
        try:
            chat_model = load_chat_model(model_name)
            
            # Create a tool-enabled version of the model if possible
            tool_enabled_model = None
            try:
                # Try to bind tools to the model for enhanced evaluation
                tool_enabled_model = chat_model.bind_tools(
                    tools=[
                        search,  # Web search tool
                        search_odoo_code,  # Odoo code search
                        load_odoo_code,  # Load specific Odoo file
                        validate_odoo_code,  # Validate against Odoo best practices
                        generate_odoo_snippet,  # Generate snippet examples
                        list_odoo_modules  # List available modules
                    ]
                )
                print(f"Successfully bound tools to {model_name} for critic evaluation")
            except NotImplementedError:
                # Model doesn't support tool binding, continue with base model
                print(f"Model {model_name} doesn't support tool binding, continuing without tools")
                tool_enabled_model = chat_model
            except Exception as e:
                # Other errors during tool binding, fall back to base model
                print(f"Error binding tools to {model_name}: {str(e)}")
                tool_enabled_model = chat_model
        except Exception as e:
            print(f"Error loading model {model_name}: {str(e)}")
            # Use a simpler approach in case of error
            def fallback_evaluator(code):
                return {
                    "score": 5.0,
                    "comment": f"Could not evaluate code: {str(e)}. Please review manually."
                }
            return fallback_evaluator
        
        # Define the evaluator function
        async def evaluator(code: str) -> Dict[str, Union[float, str]]:
            try:
                # Format the prompt with the code to evaluate
                formatted_prompt = prompt.format(outputs=code)
                
                # Create enhanced system message that mentions tool availability
                system_content = (
                    "You are an expert code evaluator for Odoo 18. Provide detailed feedback on code quality and correctness. "
                    "You have access to the following tools to help with your evaluation:\n"
                    "1. search - Search the web for information about Odoo development\n"
                    "2. search_odoo_code - Search for examples in the Odoo 18 codebase\n"
                    "3. load_odoo_code - Load specific Odoo code files\n"
                    "4. validate_odoo_code - Validate code against Odoo best practices\n"
                    "5. generate_odoo_snippet - Generate example code snippets\n"
                    "6. list_odoo_modules - List available Odoo modules\n\n"
                    "Use these tools to compare the code against Odoo standards and best practices."
                )
                
                # Create messages for the model
                messages = [
                    SystemMessage(content=system_content),
                    HumanMessage(content=formatted_prompt)
                ]
                
                # Check if this is a Google/Gemini model that might need token splitting
                is_google_model = 'google' in model_name.lower() or 'gemini' in model_name.lower()
                
                if is_google_model:
                    # Import token management utilities
                    from react_agent.token_management import (
                        should_split_messages, 
                        split_messages, 
                        get_model_token_limits
                    )
                    
                    # Check if we need to split messages based on token count
                    if should_split_messages(messages, model_name):
                        print(f"Token limit would be exceeded for critic. Splitting into smaller chunks for {model_name}")
                        chunked_messages = split_messages(messages, model_name)
                        print(f"Split into {len(chunked_messages)} chunks based on token limits")
                        
                        # Process each chunk and compile results
                        chunked_responses = []
                        for i, chunk in enumerate(chunked_messages):
                            print(f"Processing critic chunk {i+1}/{len(chunked_messages)}")
                            chunk_response = await tool_enabled_model.ainvoke(chunk)
                            if hasattr(chunk_response, 'content') and chunk_response.content:
                                chunked_responses.append(chunk_response.content)
                        
                        # Combine the chunked responses
                        if chunked_responses:
                            combined_content = "\n\n".join(chunked_responses)
                            from langchain_core.messages import AIMessage as LangChainAIMessage
                            response = LangChainAIMessage(content=combined_content)
                            content = response.content
                        else:
                            # If no chunks were successful, raise an error
                            raise ValueError("Failed to process any critic chunks successfully")
                    else:
                        # No splitting needed, invoke normally
                        response = await tool_enabled_model.ainvoke(messages)
                        content = response.content
                else:
                    # Not a Google model, invoke normally
                    response = await tool_enabled_model.ainvoke(messages)
                    content = response.content
                
                # Extract score (assuming format like "Score: 8/10" or just "8")
                score_pattern = r'(?:score|rating)[:\s]*([0-9](?:\.[0-9])?|10)(?:\s*\/\s*10)?'
                score_match = re.search(score_pattern, content, re.IGNORECASE)
                
                if score_match:
                    score = float(score_match.group(1))
                else:
                    # If no score pattern found, try to find any number between 1-10
                    number_match = re.search(r'\b([0-9](?:\.[0-9])?|10)\b', content)
                    score = float(number_match.group(1)) if number_match else 5.0
                
                # Remove score from comment to avoid redundancy
                comment = re.sub(score_pattern, '', content, flags=re.IGNORECASE)
                
                return {
                    "score": score,
                    "comment": comment.strip()
                }
            except Exception as e:
                print(f"Error during evaluation: {str(e)}")
                return {
                    "score": 5.0,
                    "comment": f"Error during evaluation: {str(e)}. Please review the code manually."
                }
        
        return evaluator

async def create_evaluator_with_fallback(prompt, model_name, is_fallback=False, max_retries=5, retry_delay=5):
    """
    Create an evaluator with fallback support and retry mechanism.
    
    Args:
        prompt: The evaluation prompt template
        model_name: The name of the model to use
        is_fallback: Whether this is already a fallback attempt
        max_retries: Maximum number of retry attempts for rate limiting or timeouts
        retry_delay: Initial delay between retries in seconds (increases with backoff)
        
    Returns:
        An async function that evaluates code
    """
    from react_agent.utils import update_model_status
    from react_agent.configuration import Configuration
    
    config = Configuration.from_context()
    
    # Adjust retry parameters based on model type - more retries and longer delays for Ollama
    is_ollama_model = 'ollama' in model_name.lower()
    if is_ollama_model and not is_fallback:
        max_retries = 5  # More retries for Ollama models
        retry_delay = 10  # Longer initial delay
        print(f"Using enhanced retry parameters for Ollama model: retries={max_retries}, initial_delay={retry_delay}s")
    
    # Attempt to create the evaluator with retries and exponential backoff
    current_delay = retry_delay
    for attempt in range(max_retries):
        try:
            # Update model status
            update_model_status("critic", "unknown", model_name, "loading")
            
            try:
                # First try the standard OpenEvals approach
                evaluator = create_llm_as_judge(prompt=prompt, model=model_name)
                update_model_status("critic", "unknown", model_name, "ready")
                return evaluator
            except NotImplementedError:
                # For models that don't support bind_tools, use our custom approach
                print(f"Model {model_name} doesn't support bind_tools, using custom judge implementation")
                # We don't need to await this since it returns a function, not a coroutine
                evaluator = await custom_llm_as_judge(prompt=prompt, model_name=model_name)
                update_model_status("critic", "unknown", model_name, "ready")
                return evaluator
                
        except Exception as e:
            error_str = str(e).lower()
            
            # Check if this is a rate limiting error
            is_rate_limit = (
                "429" in error_str or 
                "quota" in error_str or 
                "rate limit" in error_str or
                "resourceexhausted" in error_str or
                "too many requests" in error_str
            )
            
            # For rate limiting errors, retry with delay if we haven't reached max attempts
            if is_rate_limit and attempt < max_retries - 1:
                delay = retry_delay * (2 ** attempt)  # Exponential backoff
                print(f"Rate limit hit for {model_name}, retrying in {delay} seconds (attempt {attempt+1}/{max_retries})")
                update_model_status("critic", "unknown", model_name, "retrying", e)
                time.sleep(delay)
                continue
                
            # Update model status to error
            update_model_status("critic", "unknown", model_name, "error", e)
            
            # If this is already a fallback attempt and it failed, use a simple evaluator
            if is_fallback:
                print(f"Both primary and fallback models failed for critic: {str(e)}")
                # Create a very simple evaluator function as last resort
                async def simple_evaluator(code):
                    return {
                        "score": 5,  # Neutral score
                        "comment": f"Unable to evaluate code due to model errors: {str(e)}. Please review the code manually."
                    }
                return simple_evaluator
            
            # If not a fallback yet, check if fallback is enabled
            if not config.enable_critic_fallback:
                raise
            
            # Determine the appropriate fallback model
            # First try the main model, then the configured fallback model
            fallback_model = config.model
            if "google" in fallback_model.lower() or "gemini" in fallback_model.lower():
                # If main model is also Google/Gemini, use the explicit fallback model instead
                fallback_model = config.fallback_model
            
            print(f"Critic model '{model_name}' failed, falling back to '{fallback_model}': {str(e)}")
            # Update status to fallback
            update_model_status("critic", "unknown", fallback_model, "fallback", e)
            return await create_evaluator_with_fallback(prompt, fallback_model, is_fallback=True)

    # Create evaluators with fallback support
    quality_evaluator = await create_evaluator_with_fallback(ODOO_CODE_QUALITY_PROMPT, config.critic_model)
    correctness_evaluator = await create_evaluator_with_fallback(ODOO_CORRECTNESS_PROMPT, config.critic_model)

    # Define a function to generate a default evaluation when models fail
    def generate_default_evaluation(code_type="quality"):
        """Generate a default evaluation when models fail."""
        if code_type == "quality":
            return {
                "score": 5.0,
                "comment": """Quality Score: 5.0/10

The code quality could not be fully evaluated, but here are some general recommendations:

1. Ensure proper documentation for all methods and classes with docstrings
2. Follow Odoo naming conventions (snake_case for fields, methods; CamelCase for classes)
3. Organize imports properly and remove unused ones
4. Use proper indentation and formatting according to PEP8
5. Add appropriate comments for complex logic
6. Ensure field definitions include help text
7. Use appropriate field types and attributes
8. Follow Odoo's security model with proper access controls

Please review your code with these guidelines in mind."""
            }
        else:  # correctness
            return {
                "score": 5.0,
                "comment": """Correctness Score: 5.0/10

The code correctness could not be fully evaluated, but here are some general recommendations:

1. Ensure all required dependencies are properly imported
2. Verify model inheritance patterns are correctly implemented
3. Check that all required fields are defined
4. Validate that computed fields have appropriate dependencies
5. Ensure security records (access rights, record rules) are properly defined
6. Verify that views follow the correct XML structure
7. Check for proper error handling in methods
8. Ensure compatibility with Odoo 18 specific APIs

Please review your code with these guidelines in mind."""
            }
    
    # Run quality evaluation with better timeout handling
    try:
        # Update model status to running quality evaluation
        from react_agent.utils import update_model_status
        update_model_status("critic", "unknown", config.critic_model, "running_quality")
        
        # Determine if using Ollama model and adjust timeouts accordingly
        is_ollama_critic = 'ollama' in config.critic_model.lower()
        eval_timeout = config.ollama_timeout if is_ollama_critic else 60  # Use longer timeout for Ollama
        max_retries = 3 if is_ollama_critic else 1
        retry_count = 0
        retry_delay = 5  # Start with 5 seconds delay
        quality_result = None
        
        # Implement retry loop for quality evaluation
        while retry_count < max_retries and quality_result is None:
            try:
                print(f"Running quality evaluation (attempt {retry_count+1}/{max_retries}) with timeout of {eval_timeout} seconds")
                
                # Quality evaluation with appropriate timeout
                if quality_evaluator_mode == 'openevals':
                    quality_result = await asyncio.wait_for(
                        quality_evaluator(code_to_evaluate),
                        timeout=eval_timeout
                    )
                else:
                    quality_result = await asyncio.wait_for(
                        asyncio.to_thread(lambda: quality_evaluator(outputs=code_to_evaluate)),
                        timeout=eval_timeout
                    )
                    
                # Check if we got a valid result
                if not isinstance(quality_result, dict) or not quality_result.get('comment'):
                    print(f"Quality evaluation returned invalid result, will retry ({retry_count+1}/{max_retries})")
                    quality_result = None
                    retry_count += 1
                    if retry_count < max_retries:
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                        
            except (asyncio.TimeoutError, Exception) as e:
                print(f"Quality evaluation attempt {retry_count+1} failed: {str(e)}")
                retry_count += 1
                if retry_count < max_retries:
                    print(f"Retrying quality evaluation after {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
            
        # Update status to success for quality evaluation
        update_model_status("critic", "unknown", config.critic_model, "quality_complete")
            
        # Validate the result has expected structure
        if not isinstance(quality_result, dict) or not quality_result.get('comment'):
            print("Quality evaluation returned invalid result structure, using default")
            quality_result = generate_default_evaluation("quality")
            
    except (asyncio.TimeoutError, Exception) as e:
        print(f"Quality evaluation failed: {str(e)}")
        # Update status to error for quality evaluation
        from react_agent.utils import update_model_status
        update_model_status("critic", "unknown", config.critic_model, "quality_error", e)
        quality_result = generate_default_evaluation("quality")
    
    # Run correctness evaluation with better timeout handling
    try:
        # Update model status to running correctness evaluation
        from react_agent.utils import update_model_status
        update_model_status("critic", "unknown", config.critic_model, "running_correctness")
        
        # Use the same timeout and retry settings as the quality evaluation
        is_ollama_critic = 'ollama' in config.critic_model.lower()
        eval_timeout = config.ollama_timeout if is_ollama_critic else 60  # Use longer timeout for Ollama
        max_retries = 3 if is_ollama_critic else 1
        retry_count = 0
        retry_delay = 5  # Start with 5 seconds delay
        correctness_result = None
        
        # Implement retry loop for correctness evaluation
        while retry_count < max_retries and correctness_result is None:
            try:
                print(f"Running correctness evaluation (attempt {retry_count+1}/{max_retries}) with timeout of {eval_timeout} seconds")
                
                # Correctness evaluation with appropriate timeout
                if correctness_evaluator_mode == 'openevals':
                    correctness_result = await asyncio.wait_for(
                        correctness_evaluator(code_to_evaluate),
                        timeout=eval_timeout
                    )
                else:
                    correctness_result = await asyncio.wait_for(
                        asyncio.to_thread(lambda: correctness_evaluator(outputs=code_to_evaluate)),
                        timeout=eval_timeout
                    )
                    
                # Check if we got a valid result
                if not isinstance(correctness_result, dict) or not correctness_result.get('comment'):
                    print(f"Correctness evaluation returned invalid result, will retry ({retry_count+1}/{max_retries})")
                    correctness_result = None
                    retry_count += 1
                    if retry_count < max_retries:
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                        
            except (asyncio.TimeoutError, Exception) as e:
                print(f"Correctness evaluation attempt {retry_count+1} failed: {str(e)}")
                retry_count += 1
                if retry_count < max_retries:
                    print(f"Retrying correctness evaluation after {retry_delay} seconds...")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
            
        # Update status to success for correctness evaluation
        update_model_status("critic", "unknown", config.critic_model, "correctness_complete")
            
        # Validate the result has expected structure
        if not isinstance(correctness_result, dict) or not correctness_result.get('comment'):
            print("Correctness evaluation returned invalid result structure, using default")
            correctness_result = generate_default_evaluation("correctness")
            
    except (asyncio.TimeoutError, Exception) as e:
        print(f"Correctness evaluation failed: {str(e)}")
        # Update status to error for correctness evaluation
        from react_agent.utils import update_model_status
        update_model_status("critic", "unknown", config.critic_model, "correctness_error", e)
        correctness_result = generate_default_evaluation("correctness")

    # Run Odoo code validation if code blocks were found with timeout protection
    validation_result = None
    if code_blocks:
        try:
            # Update model status to running validation
            from react_agent.utils import update_model_status
            update_model_status("critic", "odoo", "validator", "running")
            
            from react_agent.odoo_code_utils import validate_odoo_code
            # Pass the first code block as a string to the tool's invoke method with timeout
            validation_result = await asyncio.wait_for(
                asyncio.to_thread(lambda: validate_odoo_code.invoke({"code": code_blocks[0]})),
                timeout=30  # 30 second timeout for validation
            )
            
            # Update status to success for validation
            update_model_status("critic", "odoo", "validator", "complete")
        except (asyncio.TimeoutError, Exception) as e:
            print(f"Odoo code validation failed: {str(e)}")
            # Update status to error for validation
            from react_agent.utils import update_model_status
            update_model_status("critic", "odoo", "validator", "error", e)
            
            validation_result = {
                "valid": False,
                "deprecation_warnings": [],
                "best_practice_suggestions": [{
                    "line": 1,
                    "message": "Validation timed out, please review your code manually",
                    "pattern": "timeout"
                }]
            }

    # Extract numerical scores using regex with error handling
    import re
    quality_score = 5.0  # Default score if extraction fails
    correctness_score = 5.0  # Default score if extraction fails
    
    try:
        quality_score_match = re.search(r'Score: (\d+(\.\d+)?)', quality_result.get('comment', ''))
        if quality_score_match:
            quality_score = float(quality_score_match.group(1))
            state.quality_score = quality_score
        else:
            # Try to find any number in the comment as a fallback
            any_number_match = re.search(r'(\d+(\.\d+)?)', quality_result.get('comment', ''))
            if any_number_match:
                quality_score = min(10.0, max(1.0, float(any_number_match.group(1))))
                state.quality_score = quality_score
            else:
                state.quality_score = quality_score
    except Exception as e:
        print(f"Error extracting quality score: {str(e)}")
        state.quality_score = quality_score
        
    try:
        correctness_score_match = re.search(r'Score: (\d+(\.\d+)?)', correctness_result.get('comment', ''))
        if correctness_score_match:
            correctness_score = float(correctness_score_match.group(1))
            state.correctness_score = correctness_score
        else:
            # Try to find any number in the comment as a fallback
            any_number_match = re.search(r'(\d+(\.\d+)?)', correctness_result.get('comment', ''))
            if any_number_match:
                correctness_score = min(10.0, max(1.0, float(any_number_match.group(1))))
                state.correctness_score = correctness_score
            else:
                state.correctness_score = correctness_score
    except Exception as e:
        print(f"Error extracting correctness score: {str(e)}")
        state.correctness_score = correctness_score

    # Adjust scores based on validation results if available
    if validation_result and not validation_result.get("valid", True):
        # Reduce quality score if deprecation warnings are found
        deprecation_count = len(validation_result.get("deprecation_warnings", []))
        if deprecation_count > 0 and hasattr(state, "quality_score"):
            # Reduce score proportionally to the number of deprecations (max reduction: 2 points)
            reduction = min(2, 0.5 * deprecation_count)
            state.quality_score = max(1, state.quality_score - reduction)

    # Format feedback with dynamic prompt for model with error handling
    try:
        quality_feedback = quality_result.get('comment', 'No quality feedback available.')
        correctness_feedback = correctness_result.get('comment', 'No correctness feedback available.')
        
        # Limit feedback length to prevent issues
        if len(quality_feedback) > 2000:
            quality_feedback = quality_feedback[:2000] + "... [truncated]"
        if len(correctness_feedback) > 2000:
            correctness_feedback = correctness_feedback[:2000] + "... [truncated]"
        
        feedback = f"""## Code Evaluation Feedback

### Quality Assessment (Score: {state.quality_score}/10)
{quality_feedback}

### Correctness Assessment (Score: {state.correctness_score}/10)
{correctness_feedback}"""
    except Exception as e:
        print(f"Error formatting feedback: {str(e)}")
        feedback = f"""## Code Evaluation Feedback

### Quality Assessment (Score: {getattr(state, 'quality_score', 5.0)}/10)
Please review your code for quality issues.

### Correctness Assessment (Score: {getattr(state, 'correctness_score', 5.0)}/10)
Please review your code for correctness issues."""

    # Add Odoo-specific validation feedback if available
    if validation_result:
        deprecation_warnings = validation_result.get("deprecation_warnings", [])
        best_practice_suggestions = validation_result.get("best_practice_suggestions", [])

        if deprecation_warnings or best_practice_suggestions:
            feedback += "\n\n### Odoo 18 Specific Validation\n"

            if deprecation_warnings:
                feedback += "\n**Deprecation Warnings:**\n"
                for i, warning in enumerate(deprecation_warnings[:5], 1):  # Limit to 5 warnings
                    feedback += f"\n{i}. Line {warning['line']}: {warning['message']} (`{warning['pattern']}`)"

            if best_practice_suggestions:
                feedback += "\n\n**Best Practice Suggestions:**\n"
                for i, suggestion in enumerate(best_practice_suggestions[:5], 1):  # Limit to 5 suggestions
                    feedback += f"\n{i}. Line {suggestion['line']}: {suggestion['message']} (`{suggestion['pattern']}`)"

    # Generate recommendations
    recommendations = [
        generate_recommendation(quality_result, correctness_result, i+1) 
        for i in range(3)
    ]
    
    try:
        # Generate recommendations with error handling
        try:
            recommendations = [
                generate_recommendation(quality_result, correctness_result, i+1) 
                for i in range(3)
            ]
            recommendations_text = "\n".join(recommendations)
        except Exception as e:
            print(f"Error generating recommendations: {str(e)}")
            recommendations_text = """1. Review your code for quality and maintainability issues.
2. Ensure your code follows Odoo best practices and standards.
3. Test your code thoroughly to identify and fix any bugs or edge cases."""
        
        # Create feedback content
        feedback_content = f"""Code Evaluation Results:

Quality Score: {quality_score}/10
Correctness Score: {correctness_score}/10

Feedback:
{quality_result.get('comment', 'Please review your code for quality issues.')}

Correctness Issues:
{correctness_result.get('comment', 'Please review your code for correctness issues.')}

Recommendations:
{recommendations_text}

Please review the feedback and make the necessary improvements to your code."""
    except Exception as e:
        print(f"Error creating feedback content: {str(e)}")
        feedback_content = """Code Evaluation Results:

Please review your code for quality and correctness issues.

Recommendations:
1. Ensure your code follows Odoo best practices and standards.
2. Test your code thoroughly to identify and fix any bugs.
3. Review the code structure and organization for maintainability."""

    # Ensure feedback content is not empty and has meaningful content
    feedback_content = feedback_content.strip()
    if not feedback_content or len(feedback_content) < 20:  # Ensure it's not just whitespace or too short
        feedback_content = "The code has been evaluated. Here are some suggestions for improvement:\n" \
                         "1. Review the code structure and organization\n" \
                         "2. Check for potential bugs or edge cases\n" \
                         "3. Ensure proper error handling is in place\n" \
                         "4. Optimize performance where possible\n" \
                         "5. Follow Odoo coding standards and best practices"

    # Create a system message for the next step
    system_message_content = f"""You are an expert Odoo developer. Your task is to improve the code based on the following feedback:

{feedback_content}

Respond with the improved code in a code block. Focus on addressing the specific issues mentioned in the feedback.

Your response should be complete and executable code that addresses all the feedback points."""

    # Ensure system message content is not empty
    system_message_content = system_message_content.strip()
    if not system_message_content or len(system_message_content) < 20:
        system_message_content = "Please improve the code based on the feedback provided. " \
                               "Focus on making the code more robust, maintainable, and efficient."

    from langchain_core.messages import SystemMessage, HumanMessage, AIMessage as LangChainAIMessage
    
    # Add error handling for the return value
    try:
        # Ensure we have valid content
        safe_feedback_content = feedback_content.strip() or "Please review the code."
        safe_system_message = system_message_content.strip() or "Please improve the code."
        
        # Limit message size to prevent issues
        if len(safe_feedback_content) > 5000:
            safe_feedback_content = safe_feedback_content[:5000] + "\n\n[Content truncated due to size]" 
        if len(safe_system_message) > 5000:
            safe_system_message = safe_system_message[:5000] + "\n\n[Content truncated due to size]"
        
        # Update status to complete for the entire critic process
        from react_agent.utils import update_model_status
        update_model_status("critic", "unknown", config.critic_model, "complete")
        
        # Track evaluation attempts to prevent infinite loops
        current_attempts = getattr(state, 'evaluation_attempts', 0) + 1
        max_attempts = 3  # Maximum number of evaluation attempts
        
        # If we've reached max attempts, add a note and mark as last step
        if current_attempts >= max_attempts:
            safe_feedback_content += "\n\nNote: Maximum evaluation attempts reached. Please review the feedback and make improvements manually."
            is_last_step = True
        else:
            is_last_step = False
            
        # Combine the feedback and system message into a single message
        combined_content = f"{safe_feedback_content}\n\nFor the next step:\n{safe_system_message}"
        
        # Calculate overall score as average of quality and correctness
        overall_score = (float(quality_score) + float(correctness_score)) / 2
        
        # Determine if code needs fixes based on scores
        needs_fixes = overall_score < 7.0
        
        # Store feedback in a format that can be used directly by call_model
        critic_feedback = safe_feedback_content
        
        # Return a single message with the combined content and all the state updates needed for regeneration
        return {
            "messages": [LangChainAIMessage(content=combined_content)],
            "is_last_step": is_last_step,
            "evaluation_attempts": current_attempts,
            "quality_score": float(quality_score),
            "correctness_score": float(correctness_score),
            "overall_score": overall_score,
            "needs_fixes": needs_fixes,
            "critic_feedback": critic_feedback,
            "critic_run_count": getattr(state, 'critic_run_count', 0) + 1
        }
    except Exception as e:
        # Fallback return value in case of errors
        print(f"Error creating return value: {str(e)}")
        # Update status to error for the entire critic process
        from react_agent.utils import update_model_status
        update_model_status("critic", "unknown", config.critic_model, "error", e)
        
        # Create a single combined message for the error case
        error_message = "An error occurred during code evaluation. Please review your code manually.\n\nFor the next step:\nPlease improve the code based on Odoo best practices."
        
        return {
            "messages": [LangChainAIMessage(content=error_message)],
            "is_last_step": True
        }


def generate_recommendation(quality_result: Dict, correctness_result: Dict, index: int) -> str:
    """Generate a recommendation based on quality and correctness evaluations.
    
    Args:
        quality_result: The quality evaluation result
        correctness_result: The correctness evaluation result
        index: The recommendation index (1-3)
        
    Returns:
        A recommendation string
    """
    # Extract comments from evaluation results
    quality_comment = quality_result.get('comment', '')
    correctness_comment = correctness_result.get('comment', '')
    
    # Look for improvement suggestions in the comments
    import re
    suggestions = []
    
    # Extract points from quality comment
    quality_points = re.findall(r'\d+\. ([^\n]+)', quality_comment)
    suggestions.extend(quality_points)
    
    # Extract points from correctness comment
    correctness_points = re.findall(r'\d+\. ([^\n]+)', correctness_comment)
    suggestions.extend(correctness_points)
    
    # If no structured points found, look for sentences with improvement keywords
    if not suggestions:
        improvement_keywords = ['improve', 'should', 'could', 'better', 'consider', 'recommend']
        for keyword in improvement_keywords:
            pattern = f'[^.!?]*{keyword}[^.!?]*[.!?]'
            suggestions.extend(re.findall(pattern, quality_comment, re.IGNORECASE))
            suggestions.extend(re.findall(pattern, correctness_comment, re.IGNORECASE))
    
    # If still no suggestions, provide generic recommendations
    generic_recommendations = [
        "Ensure proper documentation for all methods and classes",
        "Follow Odoo naming conventions for models, fields, and methods",
        "Add appropriate security checks and access controls",
        "Optimize database queries to improve performance",
        "Implement proper error handling and validation"
    ]
    
    # Return a suggestion based on the index, or a generic one if not enough suggestions
    if index <= len(suggestions):
        return suggestions[index - 1].strip()
    elif index <= len(generic_recommendations):
        return generic_recommendations[index - 1]
    else:
        return "Review your code for any additional improvements"


def extract_code_from_messages(messages: List[Any]) -> Optional[str]:
    """Extract code blocks from the conversation messages.
    
    Args:
        messages: List of conversation messages.
        
    Returns:
        str or None: Extracted code or None if no code is found.
    """
    code_blocks = []
    
    for message in messages:
        if isinstance(message, AIMessage) and not message.tool_calls:
            content = message.content
            # Simple extraction of code blocks between triple backticks
            if isinstance(content, str):
                code_start = content.find("```")
                while code_start != -1:
                    code_start += 3
                    # Skip language identifier if present
                    if content[code_start:].find("\n") != -1:
                        code_start = content.find("\n", code_start) + 1
                    
                    code_end = content.find("```", code_start)
                    if code_end != -1:
                        code_blocks.append(content[code_start:code_end].strip())
                        content = content[code_end + 3:]
                        code_start = content.find("```")
                    else:
                        break
    
    return "\n\n".join(code_blocks) if code_blocks else None